{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa21eb8-31e4-468b-9df2-158e7d6e02dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T19:37:03.276111Z",
     "iopub.status.busy": "2024-02-28T19:37:03.275164Z",
     "iopub.status.idle": "2024-02-28T19:37:03.281747Z",
     "shell.execute_reply": "2024-02-28T19:37:03.281194Z",
     "shell.execute_reply.started": "2024-02-28T19:37:03.276084Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = \"ppl\" # bleu or bleurt or ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef5a4d5-f5c7-497b-9d5f-e4518c1df673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T19:37:03.315639Z",
     "iopub.status.busy": "2024-02-28T19:37:03.314871Z",
     "iopub.status.idle": "2024-02-28T19:37:04.561509Z",
     "shell.execute_reply": "2024-02-28T19:37:04.560932Z",
     "shell.execute_reply.started": "2024-02-28T19:37:03.315616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n",
      "generated_file=/notebooks/evaluation/OWID/Llama-2-7b-hf-c2t-2-005/generated.txt\n",
      "target_file=/notebooks/evaluation/OWID/Llama-2-7b-hf-c2t-2-005/target.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "config_file = '/workspace/ChartQA/config.yaml'\n",
    "with open(config_file, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "evaluation_config = config['evaluation']\n",
    "\n",
    "generated_file = evaluation_config['generated_file']\n",
    "target_file = evaluation_config[\"target_file\"]\n",
    "print(f\"Configuration loaded successfully.\")\n",
    "print(f\"generated_file={generated_file}\")\n",
    "print(f\"target_file={target_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c60838-857e-46f4-b80f-0f1bc2d8df4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T19:37:04.562987Z",
     "iopub.status.busy": "2024-02-28T19:37:04.562739Z",
     "iopub.status.idle": "2024-02-28T19:37:04.582104Z",
     "shell.execute_reply": "2024-02-28T19:37:04.581481Z",
     "shell.execute_reply.started": "2024-02-28T19:37:04.562967Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_bleu():\n",
    "    import nltk\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    \n",
    "#     trans = bleu.load_translation_corpus(generated_file)\n",
    "#     refs = bleu.load_reference_corpus([target_file])\n",
    "\n",
    "#     bleu_score = bleu.bleu_corpus_level(trans, refs, max_order=2)\n",
    "#     print(f\"BLEU score: {bleu_score}\")\n",
    "\n",
    "    def load_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "    generated = load_file(generated_file)\n",
    "    targets = load_file(target_file)\n",
    "    \n",
    "    print(len(generated))\n",
    "    print(len(targets))\n",
    "\n",
    "    # Ensure both files have the same number of lines\n",
    "    assert len(generated) == len(targets)\n",
    "\n",
    "    # Function to calculate BLEU score\n",
    "    def calculate_bleu(generated, targets, weights):\n",
    "        references = [[t.split()] for t in targets]\n",
    "        hypothesis = [g.split() for g in generated]\n",
    "        return corpus_bleu(references, hypothesis, weights=weights)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "\n",
    "    # Calculate BLEU scores with different n-gram weights\n",
    "    bleu_1 = calculate_bleu(generated, targets, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = calculate_bleu(generated, targets, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3 = calculate_bleu(generated, targets, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4 = calculate_bleu(generated, targets, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "    print(f\"BLEU-1 score: {bleu_1}\")\n",
    "    print(f\"BLEU-2 score: {bleu_2}\")\n",
    "    print(f\"BLEU-3 score: {bleu_3}\")\n",
    "    print(f\"BLEU-4 score: {bleu_4}\")\n",
    "\n",
    "    # Example usage\n",
    "    # run_bleu(generated_file, target_file)\n",
    "\n",
    "    \n",
    "    \n",
    "def run_bleurt():\n",
    "    %cd /notebooks/bleurt\n",
    "    !pip install .\n",
    "    \n",
    "    from bleurt import score as bleurt_score\n",
    "    \n",
    "    checkpoint = \"/notebooks/bleurt/bleurt/test_checkpoint\"  # Change this to your BLEURT checkpoint path\n",
    "    scorer = bleurt_score.BleurtScorer(checkpoint)\n",
    "\n",
    "    with open(generated_file, 'r') as gen_f, open(target_file, 'r') as tar_f:\n",
    "        generated_lines = gen_f.readlines()\n",
    "        target_lines = tar_f.readlines()\n",
    "\n",
    "    bleurt_scores = scorer.score(references=target_lines, candidates=generated_lines)\n",
    "    avg_bleurt_score = sum(bleurt_scores) / len(bleurt_scores)\n",
    "    print(f\"Average BLEURT score: {avg_bleurt_score}\")\n",
    "    \n",
    "def run_ppl():\n",
    "    from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "    device = \"cuda\"  # Change to \"cpu\" if you don't have a GPU\n",
    "    model_id = \"gpt2-large\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "    def calculate_ppl(text, model, tokenizer, device):\n",
    "        encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "        max_length = model.config.n_positions\n",
    "        stride = 512\n",
    "        seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "        nlls = []\n",
    "        prev_end_loc = 0\n",
    "        for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Calculating PPL\"):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).mean())\n",
    "        return ppl.item()\n",
    "\n",
    "    with open(generated_file, 'r') as f:\n",
    "        generated_text = f.read()\n",
    "\n",
    "    ppl_score = calculate_ppl(generated_text, model, tokenizer, device)\n",
    "    print(f\"Perplexity score: {ppl_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d363b047-eb3c-4715-8382-200b7114536a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T19:37:04.583129Z",
     "iopub.status.busy": "2024-02-28T19:37:04.582936Z",
     "iopub.status.idle": "2024-02-28T20:16:55.510218Z",
     "shell.execute_reply": "2024-02-28T20:16:55.509427Z",
     "shell.execute_reply.started": "2024-02-28T19:37:04.583111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6a643fe6e74b158e0402eb7e068476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92735b2c4c649f687fb112ceb901772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f658c6c99b44358692d6ec710c9697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f92f6ef0204377b30aeceb269ad4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3075ea0b0e18451aa027bf565a55d8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8318946e841a4d9ca2b2c0663e992df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3b520082fa4e80af10a40cbc78f702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPL:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity score: 19.15102195739746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if metric == \"bleu\":\n",
    "    run_bleu()\n",
    "    \n",
    "if metric == \"bleurt\":\n",
    "    run_bleurt()\n",
    "    \n",
    "if metric == \"ppl\":\n",
    "    run_ppl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
